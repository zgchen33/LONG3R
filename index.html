<!DOCTYPE html>
<html>
<head>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <meta charset="utf-8">
  <meta name="description"
        content="LONG3R: Long Sequence Streaming 3D Reconstruction">
  <meta name="keywords" content="DUSt3R, LONG3R">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>LONG3R: Long Sequence Streaming 3D Reconstruction</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">LONG3R: Long Sequence Streaming 3D Reconstruction</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://zgchen33.github.io/">Zhuoguang Chen</a><sup>1,2*</sup>,</span>
            <span class="author-block">
              <a href="https://zgchen33.github.io/LONG3R/">Minghui Qin</a><sup>2*</sup>,</span>
            <span class="author-block">
              <a href="https://zgchen33.github.io/LONG3R/">Tianyuan Yuan</a><sup>2,3*</sup>,
            </span>
            <span class="author-block">
              <a href="https://zgchen33.github.io/LONG3R/">Zhe Liu</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://hangzhaomit.github.io/">Hang Zhao</a><sup>2,1,3â€ </sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Shanghai AI Lab</span>
            <span class="author-block"><sup>2</sup>IIIS, Tsinghua University</span>
            <span class="author-block"><sup>2</sup>Shanghai Qi Zhi Institute</span>
            <!-- <h2 class="title is-3">ICCV 2025</h2> -->
            <h2 class="title is-3" style="margin-top: 1.5rem; margin-bottom: 1.5rem;">ICCV 2025</h2>

          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="https://zgchen33.github.io/LONG3R/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <span class="link-block">
                <a href="https://arxiv.org/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/zgchen33/LONG3R/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent advancements in sparse multi-view scene reconstruction have been significant, yet existing methods face limitations 
            when processing streams of input images. These methods either rely on time-consuming offline optimization or are restricted to shorter sequences, 
            hindering their applicability in real-time scenarios. In this work, we propose LONG3R (<strong>LO</strong>ng sequence streami<strong>NG</strong> <strong>3</strong>D <strong>R</strong>econstruction), 
            a novel model designed for streaming multi-view 3D scene reconstruction over longer sequences. Our model achieves real-time processing by operating recurrently, 
            maintaining and updating memory with each new observation. We first employ a memory gating mechanism to filter relevant memory, which, together with a new observation, 
            is fed into a dual-source refined decoder for coarse-to-fine interaction. To effectively capture long-sequence memory, we propose a 3D spatio-temporal memory that 
            dynamically prunes redundant spatial information while adaptively adjusting resolution along the scene. To enhance our model&rsquo;s performance on long sequences 
            while maintaining training efficiency, we employ a two-stage curriculum training strategy, each stage targeting specific capabilities. 
            Experiments demonstrate that LONG3R outperforms state-of-the-art streaming methods, particularly for longer sequences, 
            while maintaining real-time inference speed.
          </p>          
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper Overview. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Method Overview</h2>
        <div class="content has-text-justified">
          <center>
            <img src="./static/images/main_fig.png"  width="1000">
          </center> 
          <!-- (a) Illustrates the overall architecture, where image features $F^I_t$ first interact with $F^I_{t-1}$ in the Coarse Decoder to generate $F^c_t$, 
          after which a memory-gating module filters irrelevant entries from the spatio-temporal memory $F_{\text{mem}}$. 
          The Dual-Source Refined Decoder subsequently interacts with both the filtered memory and features from $t+1$, ultimately generating the pointmap $t$.
          (b) Details the attention-based memory gating module, which selects relevant information from the memory. 
          (c) Illustrates the dual-source refined decoder, which alternately interacts with the next-frame features and relevant memory features through multiple self- 
          and cross-attention layers to optimize memory information utilization and maintain alignment with the subsequent frame. -->
          <p>
            (a) Illustrates the overall architecture, where image features \( F^I_t \) first interact with \( F^I_{t-1} \) in the Coarse Decoder to generate \( F^c_t \),
            after which a memory-gating module filters irrelevant entries from the spatio-temporal memory \( F_{\text{mem}} \).
            The Dual-Source Refined Decoder subsequently interacts with both the filtered memory and features from \( t+1 \), ultimately generating the pointmap at time \( t \).
          </p>
          <p>
            (b) Details the attention-based memory gating module, which selects relevant information from the memory.
          </p>
          <p>
            (c) Illustrates the dual-source refined decoder, which alternately interacts with the next-frame features and relevant memory features through multiple self-
            and cross-attention layers to optimize memory information utilization and maintain alignment with the subsequent frame.
          </p>
          
          <br>
        <br> 
        </div>
      </div>
    </div>
  

    <!-- Paper Results. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Results</h2>
        <div class="content has-text-justified">
          <style>
            .grid-container {
              display: grid;
              grid-template-columns: 1fr; /* Creates 2 columns with equal width */
              grid-gap: 10px; /* Adjust the gap between the divs */
            }
            .grid-item {
              width: 100%;
            }
          </style>

          <h3 class="title is-4">3D Reconstruction</h3>
          <div class="content has-text-justified">
            We evaluate the 3D reconstruction performance on the 7-scenes and NRGBD datasets.
            <br> <br>
              <center>
              <img src="./static/images/recons_table1.png"  width="500">
              </center> 
          <br> 
          </div>

          <h3 class="title is-4">Camera Pose Estimation</h3>
          <div class="content has-text-justified">
            We evaluate camera pose estimation on 7Scenes, TUM Dynamics, and ScanNet datasets.
            <br> <br>
              <center>
              <img src="./static/images/pose_table.png"  width="1000">
              </center> 
          <br> 
          </div>
        
        <h2 class="title is-3">Visualization</h2>
        <div class="content has-text-justified">
          <p>We visualize the 3D reconstruction results of LONG3R and other state-of-the-art methods.</p>
          <center><img src="./static/images/vis.jpg" width="1000"></center>
        </div>
      
      </div>
    </div> 

  </div> -->
    <!-- Paper Results and Visualization -->

    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Results</h2>
        <div class="content has-text-justified">
          <h3 class="title is-4">3D Reconstruction</h3>
          <p>We evaluate the 3D reconstruction performance on the 7-scenes and NRGBD datasets.</p>
          <center><img src="./static/images/recons_table1.png" width="1000"></center>

          <h3 class="title is-4">Camera Pose Estimation</h3>
          <p>We evaluate camera pose estimation on 7Scenes, TUM Dynamics, and ScanNet datasets.</p>
          <center><img src="./static/images/pose_table.png" width="600"></center>
        </div>

        <h2 class="title is-3">Visualization</h2>
        <div class="content has-text-justified">
          <p>We visualize the 3D reconstruction results of LONG3R and other state-of-the-art methods.</p>
          <center><img src="./static/images/vis.jpg" width="1000"></center>
        </div>
      </div>
    </div>


</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{long3r,
      title={LONG3R: Long Sequence Streaming 3D Reconstruction}, 
      author={Zhuoguang Chen and Minghui Qin and Tianyuan Yuan and Zhe Liu and Hang Zhao},
      journal={arXiv preprint arXiv:},
      year={2025}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This great website template was borrowed from <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>!
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
